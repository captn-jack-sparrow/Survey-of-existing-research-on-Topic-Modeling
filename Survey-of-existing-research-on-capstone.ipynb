{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba1b06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                               RESEARCH ON EXISTING ARTICLES ABOUT TOPIC MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d64ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7042d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this Jupyter Notebook I will be investigating two modern approaches of Topic Modeling. The purpose of doing so is to\n",
    "# gain an understanding of what the current methods are that engineers and researchers use to deploy Topic Modeling. The\n",
    "# motivation for gaining this insight is to apply these approaches to a dataset obtained from Kaggle that contains\n",
    "# 34 million Reddit comments from the subreddit WallStreetBets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "759557ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f48361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9f55140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The articles referenced in this Jupyter Notebook are fully cited at the bottom of the Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f834ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3706e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3346f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first paper that is to being examined is named \"Topic Modeling for Social Media Content: A Practical Approach\".\n",
    "# In this paper the authors explore a method of Topic Modeling called Latent Dirichlet Allocation, commonly refered\n",
    "# to as LDA. This is the most widely used method of Topic Modeling in the industry. \n",
    "# What is LDA?\n",
    "#         \" a generative probabilistic model for collections of discrete data, LDA is based on a three-level\n",
    "#         hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over\n",
    "#         an underlying set of topics. In turn, Each topic is considered as an infinite mixture over an underlying\n",
    "#         set of topic probabilities [7]. Topic models such as the LDA have become a ubiquitous and effective tool\n",
    "#         in machine learning.\"\n",
    "# (Vala, Shayaa, & Babanejaddehaki, 2016)\n",
    "# \n",
    "# so.....,\n",
    "# What is LDA?\n",
    "# LDA assumes that there are hidden topics in the documents that are influencing the words used. LDA's goal is to figure\n",
    "# out the topics and how they are distributed across the documents. It also aims to find how words are related to these\n",
    "# topics. It starts with random assignments of words to topics in the documents. Then, it iteratively refines these\n",
    "# assignments based on the distribution of words and topics in the entire corpus (AKA all documents used).\n",
    "# To perfrom LDA one can use a Library like Gensim or SciKitlearn.\n",
    "#\n",
    "# The paper uses a dataset of 90,527 records of tweets. \n",
    "# They cleaned and preprocessed the dataset as the following exerpt explains,\n",
    "#\n",
    "#          Before running LDA algorithm over 30 input datasets, we cleaned the social media data in several steps. \n",
    "#          To this end, after removing punctuations, extra spaces, and other unnecessary patterns, we created a list\n",
    "#          of English and Malay stop words including over 600 common words to get deleted from the input dataset.\n",
    "# (Vala, Shayaa, & Babanejaddehaki, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08e00b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "# (let's implement what the paper has described; plus drop NA values, NOTE:\n",
    "#    I was unable to find the dataset the  paper used, so I am using a different\n",
    "#    dataset than the one the paper uses, the dataset I am using is from Reddit\n",
    "#    so I will need to issue a profanity warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07d95825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f181ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    if isinstance(text, str):  # Check if input is a string\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str) and not pd.isnull(text):\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        filtered_text = ' '.join([word for word in tokens if word.lower() not in stop_words])\n",
    "        return filtered_text\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "    return lemmatized_text\n",
    "\n",
    "df = df.drop(df[(df['body'].isna())].index)\n",
    "\n",
    "df['body'] = df['body'].apply(remove_special_characters)\n",
    "\n",
    "df['body'] = df['body'].apply(convert_to_lowercase)\n",
    "\n",
    "df['body'] = df['body'].apply(remove_extra_whitespace)\n",
    "\n",
    "df['body'] = df['body'].apply(remove_stopwords)\n",
    "\n",
    "df['body'] = df['body'].apply(lemmatize_words)\n",
    "\n",
    "words_delete = [\"removed\", \"deleted\", \" \", \"  \", \"   \"]\n",
    "author_fullname = pd.NA\n",
    "\n",
    "\n",
    "df = df.drop(df[(df['body'].isin(words_delete)) | (df['author_fullname'].isna())].index)\n",
    "df = df.drop(df[df[\"author_fullname\"].isin(words_delete)].index)\n",
    "df = df.drop(df[(df['created_utc'].isin(words_delete)) | (df['created_utc'].isna())].index)\n",
    "df = df.drop(df[(df['author'].isin(words_delete)) | (df['author'].isna())].index)\n",
    "df = df.drop(df[(df['total_awards_received'].isin(words_delete)) | (df['total_awards_received'].isna())].index)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df['body'] = df['body'].apply(lambda x: tokenize_text(x))\n",
    "\n",
    "useless = [\"\", \"yes\", \"ban\", \"way\", \"lol\", \"f\", \"nice\", \"fuck\", \"link\", \"ok\", \"know\",\n",
    "           \"rip\", \"thanks\", \"gay\", \"nope\", \"hope\", \"guh\", \"retarded\", \"stonks, go\", \"retard\", \"position, ban\",\n",
    "           \"eat, dongus, fuckin, nerd, bot, action, performed, automatically, please, contact, moderator, subreddit, message, compose, r, wallstreetbets, question, concern\", \n",
    "          \"buy\", \"fact\", \"right\", \n",
    "           \"post, flaired, dd, dd, list, find, fresh, wsb, dd, http, n, reddit, com, r, wallstreetbets, search, sort, new, amp, q, flair, 3add, amp, restrict, sr, amp, da, misuse, dd, flair, shitposts, short, vague, guess, unexplained, news, link, etc, please, change, flair, dd, mod, notified, thread, sure, flair, use, check, guide, post, flair, http, www, reddit, com, r, wallstreetbets, wiki, linkflair, bot, action, performed, automatically, please, contact, moderator, subreddit, message, compose, r, wallstreetbets, question, concern\"\n",
    "          \"good\", \"15, monday\", \"future\", \"yep\", \"next, week\", \"always\", \"say, take, see, hold, gain, nobody, left\", \"nah\", \"bruh\", \n",
    "          \"oof\", \"real\", \"please, resubmit, shorter, title, bot, action, performed, automatically, please, contact, moderator, subreddit, message, compose, r, wallstreetbets, question, concern\", \n",
    "          \"true\", \"thank\", \"rh\", \"bb\", \"say\", \"earnings\", \"go\", \"like\", \"broken, spoke, flair, plz, mod\", \"priced\", \"tldr\", \"yessir\", \n",
    "          \"maybe\", \"sir, bread, line, bot, action, performed, automatically, please, contact, moderator, subreddit, message, compose, r, wallstreetbets, question, concern\",\n",
    "          \"username, check\", \"exactly\", \"lmao\", \"son, bitch\", \"b\", \"lolol, future, barely, green\"]\n",
    "\n",
    "df = df.drop(df[(df['body'].isin(useless))].index)\n",
    "\n",
    "final_df = df.drop(\"author_fullname\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b21e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code was not executed here, I already used this code to clean the dataset in a different notebook and I will\n",
    "# load in that file below. The reason for displaying it here is to show the exact steps taken to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa608a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a nice squeaky clean dataset lets do some LDA modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8dbac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data using enviroment variables to keep data secure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09c0e6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f85eed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpathname = os.environ.get(\"filepathname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9311b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedata = pd.read_csv(fpathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d2c61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5b2c9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.056*\"http\" + 0.053*\"com\" + 0.044*\"amp\" + 0.040*\"www\" + 0.039*\"reddit\" + 0.034*\"message\" + 0.022*\"post\" + 0.022*\"wallstreetbets\" + 0.018*\"compose\" + 0.018*\"vote\"')\n",
      "(1, '0.021*\"na\" + 0.019*\"day\" + 0.018*\"gon\" + 0.018*\"bear\" + 0.016*\"lol\" + 0.014*\"market\" + 0.014*\"go\" + 0.014*\"tesla\" + 0.014*\"green\" + 0.013*\"future\"')\n",
      "(2, '0.019*\"get\" + 0.011*\"money\" + 0.010*\"work\" + 0.009*\"people\" + 0.008*\"shit\" + 0.008*\"like\" + 0.008*\"gt\" + 0.007*\"one\" + 0.007*\"fucking\" + 0.007*\"fuck\"')\n",
      "(3, '0.014*\"market\" + 0.013*\"like\" + 0.011*\"think\" + 0.010*\"people\" + 0.009*\"would\" + 0.009*\"stock\" + 0.007*\"going\" + 0.007*\"know\" + 0.006*\"company\" + 0.006*\"year\"')\n",
      "(4, '0.028*\"call\" + 0.020*\"put\" + 0.016*\"buy\" + 0.012*\"got\" + 0.011*\"day\" + 0.010*\"get\" + 0.010*\"sell\" + 0.010*\"week\" + 0.009*\"money\" + 0.009*\"like\"')\n"
     ]
    }
   ],
   "source": [
    "documents = filedata[\"body\"]\n",
    "\n",
    "# Tokenize the documents using Gensim's simple_preprocess\n",
    "tokenized_docs = [simple_preprocess(doc) for doc in documents]\n",
    "# Assuming 'tokenized_docs' is a list of tokenized documents\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 5  # Number of topics you want to discover\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print topics and their most important words\n",
    "for topic in lda_model.show_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7ad25a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fea2deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result seems to be reasonable. The categories that the model has created does correctly\n",
    "# make some groupings around topics.\n",
    "# The first group is about links being shared, the second is about users having positive\n",
    "# and negative outlooks on the market and particularly Tesla stock. The third category seems to sum up users personal \n",
    "# philosophy. The fourth topic seems to be about people debating companies. The final topic seems to correctly categorize \n",
    "# different financial instruments like calls and puts and the terms used with them like buy and sell with a time period like\n",
    "# day or week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b2d2ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion and Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c1e7de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model approach (LDA) has decent results, but the results are not as focused as I want them to be.\n",
    "# The purpose of gaining insight into industry approaches for Topic Modeling is to apply these techniques to \n",
    "# the Reddit data and see the general topics that are discussed and the sentiment around stocks. As demonstrated above,\n",
    "# the topics that have been provided have not yeilded many stocks in their topics (the exeption of \"amp\" and \"tesla\").\n",
    "# To solve this and potentially improve this model, one approach that could be promising is called \"seed words\".\n",
    "# This is when a seed word or words are added to the dictionary for each topic. This makes the model look for topics\n",
    "# surrounding those words. \n",
    "# \n",
    "# Other approach 1:\n",
    "# Topic pruning: From an LDA outputted topic, iteratively deleting outputted words from the original dataset then\n",
    "# re-applying LDA to the resulatant dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a81f238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eaec896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "21bd1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "90b54da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second paper to be examined is not a paper but an article. The article details the process a research assistant at \n",
    "# University of Massachusetts Amherst went through to apply Topic Modeling to his University's social media. The goal of \n",
    "# doing this was to analyize the topics that got the most engagment so the social media team can take that data into\n",
    "# account in the future. \n",
    "#\n",
    "# The method for Topic Modeling described in the article is a SVM (Support Vector Machine). The researcher first\n",
    "# obtains the data and then preprocesses it by cleaning it, removing stop words, and tokenizing the text.\n",
    "# They then fit the model and view the results.\n",
    "# They also have a second function that improves the model. They do this by preprocessing\n",
    "# the data by only including data that has engagement between the 75th quantile and 100th quantile.\n",
    "# They then fit the model and look at the results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "febdd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code (Here I will recreate the steps the researcher took to make this model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c3ba55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The data used in the article is private analytics from his university's social media, so I do not have access to it.\n",
    "#       Alternatively I will use a social media comment section dataset as it is similar to the data used.\n",
    "# I will be using an unclean version of the Reddit dataset that has had no preprocessing done on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb2b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cc7b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model(file, colname, topics_num):\n",
    "    \n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    \n",
    "    filedata = pd.read_csv(file)\n",
    "    \n",
    "    df = filedata[0:305000]\n",
    "    \n",
    "        # Clean the text\n",
    "    df = df.dropna(subset=[colname])\n",
    "    df['clean_title'] = df[colname].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    df['clean_title'] = df['clean_title'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "    df['clean_title'] = df['clean_title'].apply(lambda x: x.lower())\n",
    "    \n",
    "        # deal with the stop word\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized_doc = df['clean_title'].apply(lambda x: x.split())\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "        # merge the tokenized word back to sentences again\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "    df['clean_title'] = detokenized_doc\n",
    "    \n",
    "        # vectorize it\n",
    "    vectorizer = TfidfVectorizer(max_features= 500, # keep top500 terms \n",
    "                                 max_df = 0.5, \n",
    "                                 smooth_idf=True)\n",
    "    X = vectorizer.fit_transform(df['clean_title'])\n",
    "    \n",
    "        # SVD represent documents and terms in vectors \n",
    "    svd_model = TruncatedSVD(n_components = topics_num, algorithm='randomized', n_iter=100, random_state=122)\n",
    "    svd_model.fit(X)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i, comp in enumerate(svd_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:6]\n",
    "        print(\"Topic \"+str(i)+\": \")\n",
    "        print('-------')\n",
    "        for t in sorted_terms:\n",
    "            print(t[0])\n",
    "            print(' ')\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41803f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondfpathname = os.environ.get(\"secondfilepathname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b0f32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "936e3f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "-------\n",
      "removed\n",
      " \n",
      "https\n",
      " \n",
      "reddit\n",
      " \n",
      "message\n",
      " \n",
      "wallstreetbets\n",
      " \n",
      "post\n",
      " \n",
      "Topic 1: \n",
      "-------\n",
      "deleted\n",
      " \n",
      "like\n",
      " \n",
      "post\n",
      " \n",
      "comment\n",
      " \n",
      "calls\n",
      " \n",
      "account\n",
      " \n",
      "Topic 2: \n",
      "-------\n",
      "calls\n",
      " \n",
      "like\n",
      " \n",
      "puts\n",
      " \n",
      "fuck\n",
      " \n",
      "going\n",
      " \n",
      "money\n",
      " \n",
      "Topic 3: \n",
      "-------\n",
      "calls\n",
      " \n",
      "puts\n",
      " \n",
      "bought\n",
      " \n",
      "buying\n",
      " \n",
      "sell\n",
      " \n",
      "holding\n",
      " \n"
     ]
    }
   ],
   "source": [
    "g = topic_model(secondfpathname, \"body\", 4)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99bd0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Function that is described in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "332312f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model_quantile(file, colname, metric_col, lower_quantile_no, upper_quantile_no ,topics_num):\n",
    "    \n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "       \n",
    "    df = df.dropna(subset=[colname])\n",
    "    \n",
    "    ##### Subset the data with quantile #####\n",
    "    lower_quantile, upper_quantile =   df[metric_col].quantile([lower_quantile_no/100, upper_quantile_no/100])\n",
    "    df = df.loc[(df[metric_col] > lower_quantile) & (df[metric_col] < upper_quantile)]\n",
    "    \n",
    "    df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d349cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sadly the above function cannot be performed on my dataset and get a similar result,\n",
    "# This code is meant to be implemented on a dataset that has metric data that can be differentiated\n",
    "# into quartiles.\n",
    "# My dataset has three values for metric data, 0, 1, and 2, for awards received. \n",
    "# Furthermore the amount of comments with no awards to awards has a ratio of 25 to 1.\n",
    "# the closest approximation for the desired outcome can be done in the following code.\n",
    "# (essentially just any amount of awards > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a3cd821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_models(file, colname, topics_num):\n",
    "    \n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    \n",
    "    filedata = pd.read_csv(file)\n",
    "    \n",
    "    df = filedata[filedata.total_awards_received >= 1]\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df[0:305000]\n",
    "    \n",
    "        # Clean the text\n",
    "    df = df.dropna(subset=[colname])\n",
    "    df['clean_title'] = df[colname].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    df['clean_title'] = df['clean_title'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "    df['clean_title'] = df['clean_title'].apply(lambda x: x.lower())\n",
    "    \n",
    "        # deal with the stop word\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized_doc = df['clean_title'].apply(lambda x: x.split())\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "        # merge the tokenized word back to sentences again\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "    df['clean_title'] = detokenized_doc\n",
    "    \n",
    "        # vectorize it\n",
    "    vectorizer = TfidfVectorizer(max_features= 500, # keep top500 terms \n",
    "                                 max_df = 0.5, \n",
    "                                 smooth_idf=True)\n",
    "    X = vectorizer.fit_transform(df['clean_title'])\n",
    "    \n",
    "        # SVD represent documents and terms in vectors \n",
    "    svd_model = TruncatedSVD(n_components = topics_num, algorithm='randomized', n_iter=100, random_state=122)\n",
    "    svd_model.fit(X)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i, comp in enumerate(svd_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:6]\n",
    "        print(\"Topic \"+str(i)+\": \")\n",
    "        print('-------')\n",
    "        for t in sorted_terms:\n",
    "            print(t[0])\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe2f409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "-------\n",
      "sayter\n",
      " \n",
      "type\n",
      " \n",
      "fuck\n",
      " \n",
      "awards\n",
      " \n",
      "calls\n",
      " \n",
      "elon\n",
      " \n",
      "Topic 1: \n",
      "-------\n",
      "fuck\n",
      " \n",
      "tsla\n",
      " \n",
      "calls\n",
      " \n",
      "like\n",
      " \n",
      "money\n",
      " \n",
      "going\n",
      " \n",
      "Topic 2: \n",
      "-------\n",
      "fuck\n",
      " \n",
      "holy\n",
      " \n",
      "robinhood\n",
      " \n",
      "award\n",
      " \n",
      "gold\n",
      " \n",
      "pull\n",
      " \n",
      "Topic 3: \n",
      "-------\n",
      "deleted\n",
      " \n",
      "fuck\n",
      " \n",
      "post\n",
      " \n",
      "mods\n",
      " \n",
      "shitty\n",
      " \n",
      "want\n",
      " \n"
     ]
    }
   ],
   "source": [
    "y = topic_models(secondfpathname, \"body\", 4)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a32e47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7a504087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (The below comments are on the output from the topic_models() function)\n",
    "#\n",
    "# The code has given decent results.\n",
    "# There are some patterns in the grouping of topics. For example the best category seems to be Topic 2. The topic here\n",
    "# is Robinhood the stock trading platform. Those farmiliar with Robinhood and WallStreetBets know it was very popular with \n",
    "# them during this time period before their sentiment turned on it. Regardless, the grouping in Topic 2 has \"robinhood\"\n",
    "# and \"gold\" in it. \"gold\" likely refers to the paid version of Robinhood called Robinhood Gold. It is possible that the\n",
    "# \"gold\" refers to Reddit Gold. Further analysis is required to differentiate.\n",
    "# \n",
    "# Topic 0 and Topic 1 seem to have cross over. Topic 0 includes \"elon\" and Topic 1 includes \"tsla\". There are many reasons\n",
    "# for the overlap in topics, but the most likely reason is that the subreddit talks alot about Tesla and Elon and the\n",
    "# sheer quantity of comments that contain them makes it so that it is seperable. To add to the confusion Elon is both a\n",
    "# meme and a CEO. Which are two different topics.\n",
    "#\n",
    "# (Comment below is about the difference between the performance of topic_model() and topic_models())\n",
    "# \n",
    "# The change between these two models is simply the preprocessing step of filtering for comments that gained some level\n",
    "# of \"engagment\". The first SVM model (topic_model()) has no preprocessing and the topics are not great, they get alot of \n",
    "# the bad data like \"removed\" and \"https\". But The second SVM (topic_models()) only was trained on comments that had been\n",
    "# filtered to have some \"engagment\" (ie awards). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f282504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion and Improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e2610d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The conclusion I got from this model and approach is that preprocessing the data based on some metric can prove to be \n",
    "# a very affective decision when training a model for Topic Modeling. The output was night and day when it came to \n",
    "# gaining useful topics.\n",
    "# \n",
    "# Ways to Imporve Performance:\n",
    "#\n",
    "# Using search to find the best threshold for what consitutes \"engagement\". In the code from the researcher they are looking\n",
    "# at data that has \"engagment\" between the quartiles of the 75th and 100th. And in the code performed here the threshold\n",
    "# was any comment with greater than 0 awards. With these conditions there was a boost in performance but the optimal\n",
    "# threshold could improve output. Therefore performing search over various possible values for \"engagment\" could yeild\n",
    "# better results.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc06a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c747e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "\n",
    "#1.\n",
    "#   Vala, A. R., Shayaa, S., & Babanejaddehaki, G. (2016). Topic Modeling for Social Media Content: A Practical Approach.\n",
    "#   Department of Data Analytics, Berkshire Media, Kuala Lumpur, Malaysia; Faculty of Computer Science and Information\n",
    "#   Technology, University Putra Malaysia\n",
    "\n",
    "#2.\n",
    "#   Feng, H. (2019). Decide Your Post Topics on Social Media with Simple Topic Modeling.\n",
    "#   Retrieved from https://henryfeng.medium.com/decide-your-post-topics-on-social-media\n",
    "#   -with-simple-topic-modeling-8ec1287d0eb9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d30e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference links\n",
    "\n",
    "\n",
    "# This is an academic article about using topic modeling for sentiment analysis on social media.\n",
    "# https://www.researchgate.net/publication/311755685_Topic_modeling_for_social_media_content_A_practical_approach#:~:text=In%20this%20paper%2C%20we%20explore%20an%20unsupervised%20topic,topic%20facets%20and%20extracting%20their%20dynamics%20over%20time.\n",
    "\n",
    "\n",
    "# This is an article that details the coding process of a topic model for a student researcher at a university.\n",
    "# https://henryfeng.medium.com/decide-your-post-topics-on-social-media-with-simple-topic-modeling-8ec1287d0eb9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
